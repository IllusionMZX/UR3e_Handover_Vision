#!/usr/bin/env python3

"""
Real-Time Tracking of a Green Cube using HSV Color Segmentation with Eye-in-Hand Camera on UR3e
This node detects a green cube in the camera feed, estimates its 3D position using depth data,
and commands the UR3e robot to track the cube in real-time by adjusting its TCP position.
The tracking can be done either by aligning the camera center with the cube or by aligning the
TCP center with the cube, based on user selection at startup.

Z-mapping is applied to keep the TCP at a fixed distance from the cube, the depth distance is mapped linearly to [0m,1.6m] range, 0.8m is the center point, and the range is mapped to [-0.1m, +0.1m] displacement from the initial position.

Author: Miao Zixiang
Date: 2025-12-09
"""

import rclpy
from rclpy.node import Node
from rclpy.action import ActionClient
from rclpy.callback_groups import ReentrantCallbackGroup
from rclpy.executors import MultiThreadedExecutor
from sensor_msgs.msg import Image, CameraInfo
from geometry_msgs.msg import Point
from cv_bridge import CvBridge
import cv2
import numpy as np
import tf2_ros
import tf2_geometry_msgs
import time
import math
from rtde_control import RTDEControlInterface
from rtde_receive import RTDEReceiveInterface

class HSVRealTimeTrackingNode(Node):
    def __init__(self):
        super().__init__('hsv_real_time_tracking')
        
        # Parameters
        self.target_frame = 'base_link' 
        self.camera_frame = 'camera_color_optical_frame'
        self.tool_link = 'tool0'
        
        # Callback Group for MultiThreadedExecutor
        self.callback_group = ReentrantCallbackGroup()

        # TF
        self.tf_buffer = tf2_ros.Buffer()
        self.tf_listener = tf2_ros.TransformListener(self.tf_buffer, self)
        
        # CV Bridge
        self.bridge = CvBridge()
        
        # Subscribers
        self.create_subscription(Image, '/camera/camera/color/image_raw', self.color_callback, 1, callback_group=self.callback_group)
        self.create_subscription(Image, '/camera/camera/depth/image_rect_raw', self.depth_callback, 1, callback_group=self.callback_group)
        self.create_subscription(CameraInfo, '/camera/camera/color/camera_info', self.info_callback, 1, callback_group=self.callback_group)
        
        # Publishers
        self.debug_pub = self.create_publisher(Image, '/real_time_tracking/debug_image', 1, callback_group=self.callback_group)
        
        # State
        self.latest_color = None
        self.latest_depth = None
        self.camera_intrinsics = None
        self.frame_count = 0
        self.last_update_time = 0
        self.initial_fixed_x = None # Store the initial X position
        self.initial_pos = None # Store the initial TCP position for Z-mapping
        self.is_moving = False # Flag to track robot movement status
        self.position_history = [] # For stability check
        self.stability_threshold = 0.01 # 1cm stability threshold
        
        # UR3e
        self.ur_ip = "192.168.1.10"
        self.rtde_c = RTDEControlInterface(self.ur_ip)
        self.rtde_r = RTDEReceiveInterface(self.ur_ip)
        self.get_logger().info("‚úì Â∑≤ËøûÊé•Âà∞ UR3e")
        
        # Enforce Initial Orientation
        self._enforce_initial_orientation()

        # Tracking Mode Selection
        self.tracking_mode = 'tcp' # Default to TCP center tracking
        self._select_tracking_mode()
        
        self.get_logger().info(f'=== Êú®ÂùóÂÆûÊó∂Ë∑üË∏™ËäÇÁÇπÂ∑≤ÂêØÂä® (Ê®°Âºè: {self.tracking_mode.upper()}) ===')
        self.get_logger().info('Á≠âÂæÖÁõ∏Êú∫Êï∞ÊçÆ...')

    def _enforce_initial_orientation(self):
        """
        Force the tool to a specific orientation before starting tracking.
        Step 1: Align Tool Z axis to be perpendicular to Base Z (Horizontal).
        Step 2: Align Tool X axis to be perpendicular to Base Z (Horizontal).
        Result: Tool Z and X are horizontal, Tool Y is vertical.
        """
        self.get_logger().info("Ê≠£Âú®Ë∞ÉÊï¥ÂàùÂßãÂßøÊÄÅ / Adjusting initial orientation...")
        
        # Speed settings (Slower)
        speed = 0.05
        accel = 0.05
        
        # --- Step 1: Align Z Axis (Horizontal) ---
        self.get_logger().info("Step 1: Aligning Z-axis horizontal...")
        
        # 1. Get Current Pose
        current_tcp_pose = self.rtde_r.getActualTCPPose()
        current_pos = current_tcp_pose[:3]
        current_rot = current_tcp_pose[3:]
        
        R_curr, _ = cv2.Rodrigues(np.array(current_rot))
        x_tool_curr = R_curr[:, 0]
        z_tool_curr = R_curr[:, 2]
        
        # Project Tool Z to Horizontal Plane
        z_tool_new = np.array([z_tool_curr[0], z_tool_curr[1], 0.0])
        norm_z = np.linalg.norm(z_tool_new)
        
        if norm_z < 1e-6:
            z_tool_new = np.array([-1.0, 0.0, 0.0])
        else:
            z_tool_new = z_tool_new / norm_z
            
        # Compute intermediate frame (Z aligned, X close to original)
        # We want Y_temp perpendicular to Z_new and X_curr
        y_tool_temp = np.cross(z_tool_new, x_tool_curr)
        norm_y = np.linalg.norm(y_tool_temp)
        if norm_y < 1e-6:
             y_tool_temp = np.array([0, 0, 1]) # Fallback
        y_tool_temp = y_tool_temp / np.linalg.norm(y_tool_temp)
        
        x_tool_temp = np.cross(y_tool_temp, z_tool_new)
        
        R_target_1 = np.column_stack((x_tool_temp, y_tool_temp, z_tool_new))
        rvec_1, _ = cv2.Rodrigues(R_target_1)
        rx1, ry1, rz1 = rvec_1.flatten()
        
        target_pose_1 = [current_pos[0], current_pos[1], current_pos[2], rx1, ry1, rz1]
        
        try:
            self.rtde_c.moveL(target_pose_1, speed, accel, asynchronous=False)
        except Exception as e:
            self.get_logger().error(f"Step 1 failed: {e}")
            return

        # --- Step 2: Align X Axis (Horizontal) ---
        self.get_logger().info("Step 2: Aligning X-axis horizontal...")
        
        # Update current pose (should be at target_pose_1)
        # But we use the calculated vectors from Step 1 as base to ensure consistency
        # z_tool_final is z_tool_new
        
        # We want X final to be horizontal.
        # Since Z is horizontal, X must be perpendicular to Z and horizontal.
        # This means X is parallel to the cross product of Z and World-Up (0,0,1).
        
        world_up = np.array([0.0, 0.0, 1.0])
        x_candidate = np.cross(z_tool_new, world_up) # This is horizontal
        # x_candidate is perpendicular to Z and Up.
        
        # Check direction: We want X final to be close to X temp (from Step 1)
        if np.dot(x_candidate, x_tool_temp) < 0:
            x_tool_final = -x_candidate
        else:
            x_tool_final = x_candidate
            
        x_tool_final = x_tool_final / np.linalg.norm(x_tool_final)
        
        # Recalculate Y (Will be vertical)
        y_tool_final = np.cross(z_tool_new, x_tool_final)
        
        R_target_2 = np.column_stack((x_tool_final, y_tool_final, z_tool_new))
        rvec_2, _ = cv2.Rodrigues(R_target_2)
        rx2, ry2, rz2 = rvec_2.flatten()
        
        target_pose_2 = [current_pos[0], current_pos[1], current_pos[2], rx2, ry2, rz2]
        
        try:
            self.rtde_c.moveL(target_pose_2, speed, accel, asynchronous=False)
            self.get_logger().info("ÂàùÂßãÂßøÊÄÅË∞ÉÊï¥ÂÆåÊàê / Initial orientation adjustment complete.")
        except Exception as e:
            self.get_logger().error(f"Step 2 failed: {e}")

    def _select_tracking_mode(self):
        """Ask user for tracking mode"""
        print("\n" + "="*40)
        print("ËØ∑ÈÄâÊã©Ë∑üË∏™Ê®°Âºè / Select Tracking Mode:")
        print("1. Áõ∏Êú∫‰∏≠ÂøÉÂØπÂáÜ (Camera Center Alignment)")
        print("   - ‰øùÊåÅÊú®ÂùóÂú®ÂõæÂÉè‰∏≠ÂøÉ")
        print("   - Keep object in center of image")
        print("2. Â∑•ÂÖ∑‰∏≠ÂøÉÂØπÂáÜ (TCP Center Alignment)")
        print("   - ‰øùÊåÅÂ§πÁà™‰∏≠ÂøÉÂØπÂáÜÊú®Âùó (ÊäìÂèñÈ¢ÑÂ§áÁä∂ÊÄÅ)")
        print("   - Keep TCP/Gripper center aligned with object")
        print("="*40)
        
        while True:
            try:
                choice = input("ËæìÂÖ•ÈÄâÈ°π (1/2): ").strip()
                if choice == '1':
                    self.tracking_mode = 'camera'
                    break
                elif choice == '2':
                    self.tracking_mode = 'tcp'
                    break
                else:
                    print("Êó†ÊïàËæìÂÖ•ÔºåËØ∑ËæìÂÖ• 1 Êàñ 2")
            except ValueError:
                pass
        print(f"Â∑≤ÈÄâÊã©Ê®°Âºè: {self.tracking_mode.upper()}\n")

    def info_callback(self, msg):
        if self.camera_intrinsics is None:
            self.camera_intrinsics = msg
            self.get_logger().info('‚úì Áõ∏Êú∫ÂÜÖÂèÇÂ∑≤Êé•Êî∂')

    def depth_callback(self, msg):
        self.latest_depth = msg

    def color_callback(self, msg):
        self.latest_color = msg
        self.process_frame()


    def process_frame(self):
        self.frame_count += 1
        
        # Check if robot is moving
        if self.is_moving:
            # Check actual speed to determine if movement finished
            try:
                tcp_speed = self.rtde_r.getActualTCPSpeed()
                speed_mag = np.linalg.norm(tcp_speed[:3])
                
                if speed_mag < 0.001:
                    # Robot stopped
                    current_time = time.time()
                    if current_time - self.last_update_time > 3.0:
                        self.is_moving = False
                        self.position_history = [] # Reset history after move
                        self.get_logger().info("Movement finished. Waiting 3s done. Resuming detection...")
                    else:
                        if self.frame_count % 30 == 0:
                            self.get_logger().info(f"Movement finished. Waiting... ({3.0 - (current_time - self.last_update_time):.1f}s left)")
                        return
                else:
                    if self.frame_count % 30 == 0:
                        self.get_logger().info(f"Robot is moving... Speed: {speed_mag:.4f} m/s")
                    return
            except Exception as e:
                self.get_logger().warn(f"Failed to check robot speed: {e}")
                return

        # Ê£ÄÊü•‰æùËµñÊï∞ÊçÆ
        if self.latest_color is None or self.latest_depth is None or self.camera_intrinsics is None:
            if self.frame_count % 30 == 0:
                self.get_logger().warn('Á≠âÂæÖÁõ∏Êú∫Êï∞ÊçÆ...')
            return

        # Convert images
        try:
            cv_image = self.bridge.imgmsg_to_cv2(self.latest_color, "bgr8")
            cv_depth = self.bridge.imgmsg_to_cv2(self.latest_depth, "passthrough")
            
            # Resize depth to match color if needed (Â¶ÇÊûúÁõ∏Êú∫ÈÖçÁΩÆÊ≠£Á°ÆÔºåÈÄöÂ∏∏‰∏çÈúÄË¶Å)
            if cv_image.shape[:2] != cv_depth.shape[:2]:
                cv_depth = cv2.resize(cv_depth, (cv_image.shape[1], cv_image.shape[0]), interpolation=cv2.INTER_NEAREST)
                
        except Exception as e:
            self.get_logger().error(f'CV Bridge error: {e}')
            return

        debug_img = cv_image.copy()
        hsv = cv2.cvtColor(cv_image, cv2.COLOR_BGR2HSV)



        # --- Detect Green Object (Wood Block) ---
        lower_green = np.array([40, 80, 80]) 
        upper_green = np.array([95, 255, 255])
        mask = cv2.inRange(hsv, lower_green, upper_green)
        
        kernel = np.ones((5,5),np.uint8)
        mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)
        mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)
        
        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        target_found = False
        
        if contours:
            c = max(contours, key=cv2.contourArea)
            area = cv2.contourArea(c)
            
            if area > 100: 
                target_found = True
                # Draw largest contour
                x, y, w, h = cv2.boundingRect(c)
                cv2.rectangle(debug_img, (x, y), (x+w, y+h), (0, 255, 0), 2)
                cv2.circle(debug_img, (x + w//2, y + h//2), 5, (255, 0, 0), -1)
                
                # Draw rotated rectangle
                rect = cv2.minAreaRect(c)
                box = cv2.boxPoints(rect)
                box = np.int0(box)
                cv2.drawContours(debug_img, [box], 0, (0, 0, 255), 2)
                
                M = cv2.moments(c)
                if M["m00"] != 0:
                    cx = int(M["m10"] / M["m00"])
                    cy = int(M["m01"] / M["m00"])
                    
                    # --- Robust Depth Estimation ---
                    mask_c = np.zeros_like(cv_depth, dtype=np.uint8)
                    cv2.drawContours(mask_c, [c], -1, 255, -1)
                    valid_depths = cv_depth[mask_c > 0]
                    valid_depths = valid_depths[valid_depths > 0]
                    
                    depth_val = 0
                    if len(valid_depths) > 10:
                        depth_val = np.percentile(valid_depths, 5)
                    else:
                        depth_val = cv_depth[cy, cx]
                    
                    if depth_val > 0:
                        z_meters = depth_val * 0.001
                        
                        # Deproject to 3D (Camera Frame)
                        fx = self.camera_intrinsics.k[0]
                        fy = self.camera_intrinsics.k[4]
                        ppx = self.camera_intrinsics.k[2]
                        ppy = self.camera_intrinsics.k[5]
                        
                        x_cam = (cx - ppx) * z_meters / fx
                        y_cam = (cy - ppy) * z_meters / fy  
                        z_cam = z_meters
                        
                        # Transform to Base Link
                        point_cam = tf2_geometry_msgs.PointStamped()
                        point_cam.header.frame_id = self.camera_frame
                        point_cam.header.stamp = rclpy.time.Time().to_msg()
                        point_cam.point.x = x_cam
                        point_cam.point.y = y_cam
                        point_cam.point.z = z_cam
                        
                        try:
                            point_base = self.tf_buffer.transform(point_cam, self.target_frame, timeout=rclpy.duration.Duration(seconds=0.1))
                            
                            # --- 6D Orientation Calculation (Side-View Tracking) ---
                            # We want:
                            # 1. Tool Z-axis pointing to Base -X (Forward/Backward depending on definition)
                            # 2. Tool X-axis aligned with object orientation in Y-Z plane
                            # 3. TCP X position fixed (no depth tracking)
                            # 4. TCP Y, Z tracking object
                            
                            # 1. Get Object Angle in Image
                            rect = cv2.minAreaRect(c)
                            box = cv2.boxPoints(rect)
                            box = np.int0(box)
                            
                            p0 = box[0]
                            p1 = box[1]
                            p2 = box[2]
                            d1 = np.linalg.norm(p0-p1)
                            d2 = np.linalg.norm(p1-p2)
                            
                            if d1 > d2:
                                vec_img = p1 - p0
                            else:
                                vec_img = p2 - p1
                                
                            angle_img = math.atan2(vec_img[1], vec_img[0])
                            
                            # 2. Calculate Angle in Base Frame (Y-Z Plane)
                            # We need to transform the vector from Camera to Base
                            # Construct two points in Camera frame
                            # P1 is center (already transformed as point_base)
                            # P2 is a point along the angle
                            
                            cx2 = cx + math.cos(angle_img) * 20
                            cy2 = cy + math.sin(angle_img) * 20
                            
                            x2_cam = (cx2 - ppx) * z_meters / fx
                            y2_cam = (cy2 - ppy) * z_meters / fy
                            z2_cam = z_meters
                            
                            point_cam2 = tf2_geometry_msgs.PointStamped()
                            point_cam2.header.frame_id = self.camera_frame
                            point_cam2.header.stamp = point_cam.header.stamp
                            point_cam2.point.x = x2_cam
                            point_cam2.point.y = y2_cam
                            point_cam2.point.z = z2_cam
                            
                            point_base2 = self.tf_buffer.transform(point_cam2, self.target_frame, timeout=rclpy.duration.Duration(seconds=0.1))
                            
                            # Vector in Base Frame
                            vy_base = point_base2.point.x - point_base.point.x # Not used for angle in Y-Z plane
                            vy_base = point_base2.point.y - point_base.point.y
                            vz_base = point_base2.point.z - point_base.point.z
                            
                            # Angle in Y-Z plane
                            angle_yz = math.atan2(vz_base, vy_base)
                            
                            # 3. Construct Target Rotation
                            # Tool Z = [-1, 0, 0] (Base -X)
                            # Tool X should be perpendicular to Z and aligned with object short axis
                            # Object long axis is angle_yz. Short axis is angle_yz + 90.
                            
                            theta = angle_yz + math.pi/2
                            c_t = math.cos(theta)
                            s_t = math.sin(theta)
                            
                            # R = [X_col, Y_col, Z_col]
                            # Z_col = [-1, 0, 0]
                            # X_col = [0, cos(theta), sin(theta)] (In Y-Z plane)
                            # Y_col = Z x X = [0, sin(theta), -cos(theta)]
                            
                            R_target = np.array([
                                [0,   0,   -1],
                                [c_t, s_t,  0],
                                [s_t, -c_t, 0]
                            ])
                            
                            rvec, _ = cv2.Rodrigues(R_target)
                            rx, ry, rz = rvec.flatten()

                            # 4. Calculate Target Pose
                            # Strategy: 
                            # Position: Move in Tool X/Y plane.
                            #           Keep Tool Z (depth) unchanged relative to the object.
                            # Orientation: Keep Current Orientation.
                            
                            # A. Position Calculation
                            # 1. Get Current TCP Pose (Controller Frame)
                            current_tcp_pose = self.rtde_r.getActualTCPPose()
                            current_pos = np.array(current_tcp_pose[:3])
                            current_rot_vec = np.array(current_tcp_pose[3:])
                            R_curr, _ = cv2.Rodrigues(current_rot_vec)
                            
                            # Initialize initial_pos if not set
                            if self.initial_pos is None:
                                self.initial_pos = current_pos.copy()
                                self.get_logger().info(f"Initial Position Recorded: {self.initial_pos}")

                            offset_ctrl = np.zeros(3)
                            
                            # Z-axis Depth Mapping Logic
                            # Center: 0.8m (Maps to 0 displacement from initial_pos)
                            # Range: 0m to 1.6m
                            # Output Range: -10cm to +10cm
                            # Logic: 
                            #   - Calculate target Z displacement based on z_meters
                            #   - Calculate current Z displacement from initial_pos (projected on Tool Z axis)
                            #   - Calculate required move (z_correction)
                            
                            # 1. Calculate Target Displacement (Linear Mapping)
                            # Slope = (0.1 - (-0.1)) / (1.6 - 0.0) = 0.2 / 1.6 = 0.125
                            # Formula: target_disp = 0.125 * (z_meters - 0.8)
                            target_z_disp = 0.125 * (z_meters - 0.8)
                            
                            # 2. Clamp Target Displacement (-10cm to +10cm)
                            # Also handles z_meters > 1.6m (maintains +10cm)
                            target_z_disp = max(min(target_z_disp, 0.1), -0.1)
                            
                            # 3. Calculate Current Displacement along Tool Z Axis
                            # Tool Z axis in Controller Frame is the 3rd column of R_curr
                            tool_z_axis = R_curr[:, 2]
                            
                            # Vector from Initial Pos to Current Pos
                            diff_pos = current_pos - self.initial_pos
                            
                            # Project onto Tool Z axis
                            current_z_disp = np.dot(diff_pos, tool_z_axis)
                            
                            # 4. Calculate Required Move (Step)
                            z_correction = target_z_disp - current_z_disp
                            
                            if self.tracking_mode == 'camera':
                                # --- Mode 1: Camera Center Alignment ---
                                # We want to move the tool such that the object (currently at x_cam, y_cam) moves to (0,0) in Camera Frame.
                                # So we shift the tool by (x_cam, y_cam, z_correction) in Tool Frame.
                                # Note: This assumes Camera Frame axes are aligned with Tool Frame axes (which is roughly true for Eye-in-Hand)
                                
                                offset_tool = np.array([x_cam, y_cam, z_correction])
                                offset_ctrl = R_curr @ offset_tool
                                
                            else: # self.tracking_mode == 'tcp'
                                # --- Mode 2: TCP Center Alignment ---
                                # We want the TCP to move such that the object is at (0,0) in TCP Frame.
                                
                                # Construct Homogeneous Transform Matrix for TCP (T_ctrl_tcp)
                                T_ctrl_tcp = np.eye(4)
                                T_ctrl_tcp[:3, :3] = R_curr
                                T_ctrl_tcp[:3, 3] = current_pos
                                
                                # Get Object Position in Controller Frame
                                # Mapping: X_ctrl = -X_ros, Y_ctrl = -Y_ros, Z_ctrl = Z_ros
                                P_obj_ctrl = np.array([
                                    -point_base.point.x,
                                    -point_base.point.y,
                                    point_base.point.z,
                                    1.0
                                ])
                                
                                # Transform Object to TCP Frame
                                # P_obj_tcp = inv(T_ctrl_tcp) * P_obj_ctrl
                                T_tcp_ctrl = np.linalg.inv(T_ctrl_tcp)
                                P_obj_tcp = T_tcp_ctrl @ P_obj_ctrl
                                
                                # Calculate Offset in TCP Frame
                                # Required Move in TCP: (x, y, z_correction)
                                offset_tcp = np.array([P_obj_tcp[0], P_obj_tcp[1], z_correction])
                                
                                # Transform Offset to Controller Frame
                                offset_ctrl = R_curr @ offset_tcp
                            
                            # 6. New Target Position
                            target_pos_ctrl = current_pos + offset_ctrl
                            
                            # B. Orientation Calculation (Keep Current Orientation)
                            target_pose = [
                                target_pos_ctrl[0],
                                target_pos_ctrl[1],
                                target_pos_ctrl[2],
                                current_rot_vec[0], 
                                current_rot_vec[1], 
                                current_rot_vec[2],
                            ]
                            
                            # Check reachability (UR3e max reach ~0.5m + Tool Length)
                            dist_from_base = math.sqrt(target_pose[0]**2 + target_pose[1]**2 + target_pose[2]**2)
                            
                            # Stability Check
                            self.position_history.append(target_pose[:3])
                            if len(self.position_history) > 10:
                                self.position_history.pop(0)
                            
                            is_stable = False
                            if len(self.position_history) >= 10:
                                xs = [p[0] for p in self.position_history]
                                ys = [p[1] for p in self.position_history]
                                zs = [p[2] for p in self.position_history]
                                max_dev = max(max(xs)-min(xs), max(ys)-min(ys), max(zs)-min(zs))
                                
                                if max_dev < self.stability_threshold:
                                    is_stable = True
                                else:
                                    if self.frame_count % 30 == 0:
                                        self.get_logger().info(f"Waiting for stability... Dev: {max_dev:.4f}m")

                            if is_stable:
                                # Move to target (Slow speed)
                                # asynchronous=True to avoid blocking the camera callback
                                self.rtde_c.moveL(target_pose, 0.05, 0.05, asynchronous=True)
                                self.last_update_time = time.time()
                                self.is_moving = True
                                self.get_logger().info(f"Executing move... Target: {target_pose} (Dist: {dist_from_base:.2f}m)")
                            
                            # Logging
                            if self.frame_count % 30 == 0 and not is_stable:
                                try:
                                    tool0_pos = None # Not querying to save time/complexity if not needed
                                    self._log_detection_details(cv_depth.shape, cx, cy, ppx, ppy, z_meters, fx, fy, x_cam, y_cam, z_cam, tool0_pos, None, point_base)
                                    self.get_logger().info(f"Angle Y-Z: {math.degrees(angle_yz):.1f} deg")
                                    self.get_logger().info(f"Mode: {self.tracking_mode} | Target Pose (Controller): {target_pose}")
                                except Exception as e:
                                    self.get_logger().warn(f"Logging error: {e}")

                        except Exception as e:
                            self.get_logger().error(f'Transform/Control Error: {e}')
        
        if not target_found:
            # Stop if lost tracking? Or just stay?
            # self.rtde_c.servoStop()
            pass

        self._publish_debug_image(debug_img)
    
    def _log_detection_details(self, cv_depth_shape, cx, cy, ppx, ppy, z_meters, fx, fy, x_cam, y_cam, z_cam, tool0_pos, tool0_rot, point_base):
        """ÊâìÂç∞ËØ¶ÁªÜÁöÑÊ£ÄÊµãÂíåÂùêÊ†áËΩ¨Êç¢‰ø°ÊÅØ"""
        self.get_logger().info("=" * 60)
        self.get_logger().info("„ÄêÂùêÊ†áËΩ¨Êç¢ËØ¶ÁªÜ‰ø°ÊÅØ„Äë")
        self.get_logger().info(f"1. ÂõæÂÉèÂ∞∫ÂØ∏: {cv_depth_shape[1]}x{cv_depth_shape[0]} ÂÉèÁ¥†")
        self.get_logger().info(f"2. Ê£ÄÊµãÂà∞ÁöÑÂõæÂÉèÂùêÊ†á: cx={cx}, cy={cy}")
        self.get_logger().info(f"3. Áõ∏Êú∫ÂÖâÂ≠¶‰∏≠ÂøÉ: ppx={ppx:.1f}, ppy={ppy:.1f}")
        self.get_logger().info(f"4. ÂÉèÁ¥†ÂÅèÁßªÈáè: dx={cx-ppx:.1f}, dy={cy-ppy:.1f}")
        self.get_logger().info(f"5. Ê∑±Â∫¶ÂÄº: z={z_meters:.4f}m")
        self.get_logger().info(f"6. Áõ∏Êú∫ÂÜÖÂèÇ: fx={fx:.1f}, fy={fy:.1f}")
        self.get_logger().info(f"7. Áõ∏Êú∫ÂùêÊ†áÁ≥ª (camera_color_optical_frame):")
        self.get_logger().info(f"   X={x_cam:.4f}m, Y={y_cam:.4f}m, Z={z_cam:.4f}m")
        
        if tool0_pos is not None and tool0_rot is not None:
            self.get_logger().info("=" * 60)
            self.get_logger().info("„ÄêTFÂèòÊç¢Èìæ‰ø°ÊÅØ„Äë")
            self.get_logger().info(f"8. ÂΩìÂâç tool0 ‰ΩçÁΩÆ (Áõ∏ÂØπ base_link):")
            self.get_logger().info(f"   X={tool0_pos.x:.4f}m, Y={tool0_pos.y:.4f}m, Z={tool0_pos.z:.4f}m")
            self.get_logger().info(f"   ÂõõÂÖÉÊï∞: [{tool0_rot.x:.4f}, {tool0_rot.y:.4f}, {tool0_rot.z:.4f}, {tool0_rot.w:.4f}]")
        
        if point_base is not None:
            self.get_logger().info('='*60)
            self.get_logger().info(f'üéØ Ê£ÄÊµãÂà∞Êú®Âùó!')
            self.get_logger().info(f"10. base_link ÂùêÊ†áÁ≥ª:")
            self.get_logger().info(f'    X={point_base.point.x:.4f}m, Y={point_base.point.y:.4f}m, Z={point_base.point.z:.4f}m')
            self.get_logger().info('='*60)

    def _publish_debug_image(self, cv_image):
        """ÂèëÂ∏ÉË∞ÉËØïÂõæÂÉè"""
        try:
            debug_msg = self.bridge.cv2_to_imgmsg(cv_image, "bgr8")
            self.debug_pub.publish(debug_msg)
        except Exception as e:
            self.get_logger().error(f'ÂèëÂ∏ÉË∞ÉËØïÂõæÂÉèÂ§±Ë¥•: {e}')

    def destroy_node(self):
        # Á°Æ‰øùËÑöÊú¨ÂÅúÊ≠¢ÔºåÈáäÊîæÊú∫Ê¢∞ËáÇÊéßÂà∂ÊùÉ
        try:
            if hasattr(self, 'rtde_c'):
                self.rtde_c.servoStop()
                self.rtde_c.stopScript()
        except Exception as e:
            self.get_logger().warn(f"ÈáäÊîæÊú∫Ê¢∞ËáÇÊéßÂà∂ÊùÉÂ§±Ë¥•: {e}")
        super().destroy_node()

def main(args=None):
    rclpy.init(args=args)
    node = HSVRealTimeTrackingNode()
    
    executor = MultiThreadedExecutor()
    executor.add_node(node)
    
    try:
        executor.spin()
    except KeyboardInterrupt:
        pass
    finally:
        node.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()
